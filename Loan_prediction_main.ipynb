{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c269cc-1ca7-4c05-b27e-4e5c277628cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('future.no_silent_downcasting', True)\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from xgboost import XGBClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import shap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4eda532-4593-4bda-8239-eebdea0d0316",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a04627-1e01-4517-a433-09d00a8e38e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "train_data = pd.read_csv('Data/Loan_dataset_train.csv') # This is the portion of the dataset used to \"train\" or fit the model.\n",
    "test_data = pd.read_csv('Data/Loan_dataset_test.csv') # This subset is reserved for evaluating the model’s performance after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a61398-1a4e-4a56-8386-c3f30f92a8eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Display the first few rows of the training dataset\n",
    "print(\"Training Data Head:\")\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca59fc8-940d-4e78-9deb-106a2e7e90b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows of the test dataset\n",
    "print(\"\\nTest Data Head:\")\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbae21f-64ca-4560-a606-240640872501",
   "metadata": {},
   "source": [
    "## Understanding the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f831a066-e44b-4b0d-bca5-927d361f2d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data columns\n",
    "train_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4fda37-e997-4366-a00f-d600986bdaf2",
   "metadata": {},
   "source": [
    "There are 12 independent variables and 1 target variable, i.e., `Loan_Status`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f3b76c-7f60-49cd-a8c8-46799817be70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data columns\n",
    "test_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414be230-4cd5-446f-8974-a906d6c7f534",
   "metadata": {},
   "source": [
    "The test dataset contains the same features as the training dataset, except for `Loan_Status`. The model will be trained on the training data to predict `Loan_Status` for the test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377156b0-b541-4602-99e1-d6eb8b13b227",
   "metadata": {},
   "source": [
    "The description of each variable is provided below.\n",
    "\n",
    "| Variable           | Description                                  |\r\n",
    "|--------------------|----------------------------------------------|\r\n",
    "| Loan_ID            | Unique Loan ID                               |\r\n",
    "| Gender             | Male/ Female                                 |\r\n",
    "| Married            | Applicant married (Y/N)                      |\r\n",
    "| Dependents         | Number of dependents                         |\r\n",
    "| Education          | Applicant Education (Graduate/Under Graduate)|\r\n",
    "| Self_Employed      | Self employed (Y/N)                          |\r\n",
    "| ApplicantIncome    | Applicant income                             |\r\n",
    "| CoapplicantIncome  | Coapplicant income                           |\r\n",
    "| LoanAmount         | Loan amount in thousands                     |\r\n",
    "| Loan_Amount_Term   | Term of loan in months                       |\r\n",
    "| Credit_History     | Creof individual’s repayment of their debts guidelines              |\r\n",
    "| Property_Area      | Urban/ Semi Urban/ Rural                     |\r\n",
    "| Loan_Status        | Loan approved (Y/N)                          |\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba47aec3-22d7-494c-8805-4830ec7b40a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset shape\n",
    "train_data.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323585b5-8595-4997-9a50-f98e474bfee6",
   "metadata": {},
   "source": [
    "The training dataset contains 614 rows and 13 columns, while the test dataset includes 367 rows and 12 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef2ca73-f0c3-4479-ad9f-f69c8d70ae80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the Loan_ID variable since it doesn't impact the loan status. \n",
    "train_data = train_data.drop('Loan_ID', axis=1)\n",
    "test_data = test_data.drop('Loan_ID', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e89402-9562-4c57-8091-d5b5f4cdc88b",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "To enhance the model's predictive power, new features are introduced based on domain knowledge (for more details see, e.g., these two conference papers [[1]](https://doi.org/10.1051/itmconf/20224403019) and [[2]](https://link.springer.com/chapter/10.1007/978-981-99-0601-7_14)). These features are designed to influence the target variable, loan approval likelihood, as follows:\n",
    "\n",
    "1. **Total Income**  \n",
    "   - By combining the `Applicant Income` and `Co-applicant Income`, we create a `Total Income` feature. Higher total income may correlate with a higher probability of loan approval, as it indicates greater financial capability. \n",
    "\n",
    "2. **EMI (Equated Monthly Installment)**  \n",
    "   - EMI represents the fixed monthly payment required to repay the loan over the specified term. EMI is the ratio of the loan amount to the loan term. Applicants with a high EMI might experience more financial strain, potentially lowering their ability to keep up with payments.\n",
    "\n",
    "3. **Balance Income**  \n",
    "   - This feature captures the income remaining after the EMI has been deducted. We hypothesize that a higher balance income enhances the likelihood of loan repayment, as it suggests sufficient funds are available even after covering loan payments, increasing the chances of loan approval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fd5cde-9364-4508-be0c-4d47ed4de809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total Income\n",
    "train_data['Total_Income']=train_data['ApplicantIncome']+train_data['CoapplicantIncome']\n",
    "test_data['Total_Income']=test_data['ApplicantIncome']+test_data['CoapplicantIncome']\n",
    "\n",
    "# EMI\n",
    "train_data['EMI']=train_data['LoanAmount']/train_data['Loan_Amount_Term'] \n",
    "test_data['EMI']=test_data['LoanAmount']/test_data['Loan_Amount_Term']\n",
    "\n",
    "# Balance Income\n",
    "train_data['Balance_Income']=train_data['Total_Income']-(train_data['EMI']*1000) \n",
    "test_data['Balance_Income']=test_data['Total_Income']-(test_data['EMI']*1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50aef685-4620-47f2-a0bf-8e0d82c804ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy of the data\n",
    "train_data_original = train_data.copy()\n",
    "test_data_original = test_data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee2f327-74fe-484b-8dcd-1f18fda5437f",
   "metadata": {},
   "source": [
    "Remove the variables used to create these new features, as they will likely be highly correlated with the new features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac18527-cdb1-45c3-9116-ed9cdc1d5dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=train_data.drop(['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount', 'Loan_Amount_Term'], axis=1)\n",
    "test_data=test_data.drop(['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount', 'Loan_Amount_Term'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e980592-6d74-49f6-978a-430e8cd66cac",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "Expanding on the exploratory analysis will allow to better understand the data distribution, feature relationships, and key patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d644b592-776b-4c32-ac83-f71a2bde34e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart and pie chart\n",
    "fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "\n",
    "# Bar chart for 'Loan_Status' distribution\n",
    "\n",
    "# Counting the values for 'Loan_Status'\n",
    "loan_status_counts = train_data['Loan_Status'].value_counts()\n",
    "# Calculating percentages for pie chart\n",
    "total = loan_status_counts.sum()\n",
    "loan_status_percentages = (loan_status_counts / total) * 100\n",
    "\n",
    "loan_status_counts.plot.bar(ax=axes[0], color=['skyblue', 'lightcoral'], edgecolor='black')\n",
    "axes[0].set_title('Distribution of Loan Status (Y/N) - Bar Chart')\n",
    "axes[0].set_xlabel('Loan Status')\n",
    "axes[0].set_ylabel('Count')\n",
    "\n",
    "# Pie chart for 'Loan_Status' distribution\n",
    "axes[1].pie(loan_status_percentages, labels=loan_status_counts.index, autopct='%1.1f%%', startangle=90, \n",
    "            colors=['skyblue', 'lightcoral'], wedgeprops={'edgecolor': 'black'})\n",
    "axes[1].set_title('Distribution of Loan Status (Y/N) - Pie Chart')\n",
    "\n",
    "# Displaying the combined plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872e8fe5-3830-4956-b90c-d4d209969f88",
   "metadata": {},
   "source": [
    "The distribution plot shows that the target variable, `Loan_Status`, has more approved loans (labeled as Y) than disapproved ones (labeled as N). 68.7% had received loan approval, while 31.3% were rejected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24036419-91c8-4385-b1f3-92009b18d641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distributions of numerical features\n",
    "\n",
    "# List of numerical columns\n",
    "numerical_columns = ['Total_Income', 'EMI', 'Balance_Income'] \n",
    "\n",
    "# Loop through each column to create distribution and boxplots\n",
    "for column in numerical_columns:\n",
    "    plt.figure(figsize=(16, 5))\n",
    "    \n",
    "    # Distribution plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.histplot(train_data[column], kde=True)  \n",
    "    plt.title(f'Distribution of {column}')\n",
    "    \n",
    "    # Boxplot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    train_data[column].plot.box()\n",
    "    plt.title(f'Boxplot of {column}')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d0a92b-6c1b-4106-a789-595b62f457d9",
   "metadata": {},
   "source": [
    "The histograms show that their distribution are right-skewed, indicating that a few applicants have significantly higher incomes than others. This skewness might necessitate scaling or transformation for more effective modeling. There are also some outliers that will be addressed in later sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c791c1d-d70f-4490-9707-2c2c624bd10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of categorical columns\n",
    "categorical_features = ['Gender', 'Married', 'Dependents', 'Education', 'Self_Employed', 'Property_Area', 'Credit_History']\n",
    "\n",
    "# Set up the figure for plotting with increased height\n",
    "plt.figure(figsize=(15, 12))\n",
    "\n",
    "# Loop through each categorical feature\n",
    "for i, column in enumerate(categorical_features, 1):\n",
    "    plt.subplot(3, 3, i)\n",
    "    \n",
    "    # Set a unique color palette for each feature using a Seaborn color palette\n",
    "    unique_values = train_data[column].nunique()\n",
    "    palette = sns.color_palette(\"Set2\", unique_values)  # Choose a different palette if preferred\n",
    "    \n",
    "    # Plot count plot with assigned colors and hue\n",
    "    ax = sns.countplot(data=train_data, x=column, hue=column, palette=palette, dodge=False)\n",
    "    plt.title(f'Distribution of {column}')\n",
    "    \n",
    "    # Check if legend exists and remove it if present\n",
    "    legend = ax.get_legend()\n",
    "    if legend is not None:\n",
    "        legend.remove()\n",
    "    \n",
    "    # Calculate percentages and annotate, skipping zero percentages\n",
    "    total = len(train_data)\n",
    "    for p in ax.patches:\n",
    "        count = p.get_height()\n",
    "        percentage = 100 * count / total\n",
    "        if percentage > 0:  # Only annotate if percentage is greater than zero\n",
    "            x = p.get_x() + p.get_width() / 2\n",
    "            y = p.get_height()\n",
    "            ax.annotate(f'{percentage:.1f}%', (x, y), ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9880d1-dded-43c3-a31e-17794f99d020",
   "metadata": {},
   "source": [
    "The count plots reveal key characteristics of the categorical features:\n",
    "\n",
    "- `Gender` and `Married`: The majority of applicants are male and married.\n",
    "- `Dependents`: There is a higher proportion of applicants without dependents, with fewer applicants reporting multiple dependents.\n",
    "- `Education`: Most applicants are graduates.\n",
    "- `Self_Employed`: Fewer applicants are self-employed, which might be a distinguishing characteristic among applicants.\n",
    "- `Credit_History`: The majority of applicants have repaid their debts\n",
    "- `Property_Area`: The distribution is fairly balanced across *Rural*, *Urban*, and *Semiurban* areas, potentially aiding in differentiating loan outcomes based on geographic context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b296bc8-137b-4946-9c23-da35fc68f4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set color palette to only the first two colors of 'Set2'\n",
    "palette = sns.color_palette(\"Set2\")[:2]\n",
    "\n",
    "# Countplots for categorical features with respect to target\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, column in enumerate(categorical_features, 1):\n",
    "    plt.subplot(3, 3, i)\n",
    "    sns.countplot(data=train_data, x=column, hue='Loan_Status', palette=palette)  # Use palette here\n",
    "    plt.title(f'{column} vs Target')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b952f5ff-2d69-4aa2-b25e-cfb3ecf64602",
   "metadata": {},
   "source": [
    "The count plots for categorical features reveal key relationships with loan approval status (`Loan_Status`). Here’s a summary of the insights:\n",
    "\n",
    "- Male and married applicants have slightly higher loan approval rates, though this difference is minor.\n",
    "- Applicants with fewer dependents appear to have slightly better approval rates.\n",
    "- Graduates show higher loan approval rates than non-graduates.\n",
    "- Non-self-employed applicants have higher approval rates.\n",
    "- Applicants with a credit history (recorded as 1.0) show a significantly higher loan approval rate, indicating it is a strong predictor.\n",
    "- Approval rates are higher in *Semiurban* areas compared to *Urban* and *Rural* areas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e226d93c-d4f0-4735-81af-2472f2f617d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c6d3fd-0024-46b5-ab5a-8e79cd0b039e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categrical variables into numerical value\n",
    "\n",
    "le = LabelEncoder()\n",
    "for col in categorical_features:\n",
    "    train_data[col] = le.fit_transform(train_data[col])\n",
    "    test_data[col] = le.transform(test_data[col])\n",
    "\n",
    "train_data['Loan_Status'] = le.fit_transform(train_data['Loan_Status'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c49646-a73d-4bda-b8ab-eb912caaae35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix \n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(train_data.corr(), annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Correlation Matrix of Numerical Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c985fb-35a1-4426-a173-aec2ddc05b7d",
   "metadata": {},
   "source": [
    "The correlation matrix provides insights into the relationships between the features in the dataset with a focus on predicting the `Loan_Status`. Here are the key observations:\n",
    "\n",
    "1. **Loan Status Correlations**:\n",
    "   - `Credit_History` shows the highest positive correlation with `Loan_Status`, suggesting that credit history is a significant factor in determining loan approval.\n",
    "   - Most other features have very low correlation with `Loan_Status`. For instance, `Total_Income`, `Balance_Income` and `Married` have very low correlation values, indicating that they are not strongly related to the loan approval outcome.\n",
    "   - `Education`, `Self_Employed` and `Property_Area` have almost negligible correlation values with `Loan_Status`, suggesting they have minimal or no predictive power for this target variable in this dataset.\n",
    "  \n",
    "2. **Feature Interdependencies**:\n",
    "   - `Total_Income` has a moderate positive correlation with `EMI`, indicating that as total income increases, EMI values also tend to increase, which is expected since individuals with higher incomes may be eligible for larger loans.\n",
    "   - `Married` shows a mild positive correlation with `Gender` and `Dependents`, which could reflect a demographic pattern within the dataset where married individuals might often have dependents and be of a certain gender distribution.\n",
    "\n",
    "3. **Insights for Predictive Modeling**:\n",
    "   - Given the high correlation between `Credit_History` and `Loan_Status`, `Credit_History` should be a primary feature in the model.\n",
    "   - Other features with minimal correlation may not significantly contribute to predicting `Loan_Status` and could be reconsidered during feature selection to snr larger loan amounts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e555643-2304-469e-a34d-f007bbdffe61",
   "metadata": {},
   "source": [
    "## Missing Value and Outlier Treatment\r\n",
    "Identifying missing values and outliers aims to improve data quality by ensuring accuracy and reliability, as these issues can skew analysis and lead to incorrect conclusions. Addressing them helps build robust models and derive meaningful insights from the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddac53d-c242-4f82-987e-2a0911af7515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "train_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9dceb3a-db22-4eb7-9d07-3da68f54a003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "test_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23694498-f806-4533-bf94-287d3122c176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill or drop missing values as necessary\n",
    "# For example, filling categorical missing values with the mode and numerical with the median\n",
    "for column in train_data.columns:\n",
    "    if column in  categorical_features:\n",
    "        train_data[column] = train_data[column].fillna(train_data[column].mode()[0])\n",
    "        test_data[column] = train_data[column].fillna(train_data[column].mode()[0])\n",
    "    else:\n",
    "        train_data[column] = train_data[column].fillna(train_data[column].median())\n",
    "        test_data[column] = train_data[column].fillna(train_data[column].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f6f3de-a495-4e22-b856-0fcc1f4edb13",
   "metadata": {},
   "source": [
    "Apply the log transformation to right-skewed column distributions: smaller values are only minimally impacted, while larger values are substantially reduced, resulting in a distribution that more closely resembles a normal distribution. This transformation mitigates the influence of extreme values, making the distribution more symmetric and suitable for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb9faa9-a8fe-4761-adef-85efc8e44018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total Income\n",
    "train_data['Total_Income_log'] = np.log(train_data['Total_Income']) \n",
    "train_data['Total_Income_log'].hist(bins=20) # effect of log transformation\n",
    "test_data['Total_Income_log'] = np.log(test_data['Total_Income'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8787571-4daf-4401-a787-26e41c1100c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EMI\n",
    "train_data['EMI_log'] = np.log(train_data['EMI']) \n",
    "train_data['EMI_log'].hist(bins=20) # effect of log transformation\n",
    "test_data['EMI_log'] = np.log(test_data['EMI'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c2799d-dad9-4b04-9fd0-b0d7c7d05c8a",
   "metadata": {},
   "source": [
    "`Balance_Income` include two negative values. Since a log transformation requires positive values, a constant equal to the absolute value of the minimum value of the balance income in the training and test datasets, plus 1, can be added to shift the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7089f599-c105-43a5-a0d6-9045a4ab29e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CoapplicantIncome\n",
    "min_value = min(train_data['Balance_Income'].min(), test_data['Balance_Income'].min())\n",
    "c = abs(min_value) + 1\n",
    "train_data['Balance_Income_log'] = np.log(train_data['Balance_Income'] + c) \n",
    "train_data['Balance_Income_log'].hist(bins=20) # effect of log transformation\n",
    "test_data['Balance_Income_log'] = np.log(test_data['Balance_Income'] + c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065938a9-f8d4-44e2-b11d-c1376d5e9c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analisi outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f74277-6b48-4fd7-9514-7e4477c0c9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the feature before log-transformation\n",
    "train_data=train_data.drop(['Balance_Income', 'Total_Income', 'EMI'], axis=1)\n",
    "test_data=test_data.drop(['Balance_Income', 'Total_Income', 'EMI'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f2c70c-48c0-495c-b340-5ba3db360786",
   "metadata": {},
   "source": [
    "## Machine Learning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0ea00e-49a3-4f41-9191-a5f513099a1d",
   "metadata": {},
   "source": [
    "When you have an imbalanced dataset, accuracy may not be the best metric to evaluate a Machine Learning (ML) model. A model that predicts only the majority class (1) would achieve about 70% accuracy without actually learning anything meaningful about the minority class (0). Furthermore, accuracy does not account for the distribution between the two classes, making it less informative in imbalanced scenarios.\n",
    "\n",
    "F1 Score will be used to compare the performancce of differente ML model and is defined as the harmonic mean of precision and recall.\n",
    "Precision tells you how often the model is correct when it predicts class 1, whereas, recall helps to see how well the model is identifying class 1 correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155dd5d9-76b5-4654-a6ed-c4ecdf0879ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for feeding into the model\n",
    "X = train_data.drop('Loan_Status', axis=1) \n",
    "X_test = test_data.copy()\n",
    "y = train_data.Loan_Status.astype(int)\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Apply the scaler only to the numerical columns in X\n",
    "numerical_columns_log = ['Total_Income_log', 'EMI_log', 'Balance_Income_log']\n",
    "X[numerical_columns_log] = scaler.fit_transform(X[numerical_columns_log])\n",
    "X_test[numerical_columns_log] = scaler.transform(X_test[numerical_columns_log])\n",
    "\n",
    "# Define 10-fold stratified cross-validator\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=27)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da89616-1af5-47a6-bc0d-7a360cead702",
   "metadata": {},
   "source": [
    "A K-Nearest Neighbors model is trained as a baseline to compare the performance against more advanced models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb672c5-7e63-4faa-a060-a05d0c23d745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Nearest Neighbors (KNN)\n",
    "knn_params = {\n",
    "    'n_neighbors': [3, 5, 7, 9, 11],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan']\n",
    "}\n",
    "\n",
    "knn_model = KNeighborsClassifier()\n",
    "knn_grid = GridSearchCV(knn_model, knn_params, cv=skf, scoring='f1')\n",
    "knn_grid.fit(X, y)\n",
    "\n",
    "print(\"Best KNN Parameters:\", knn_grid.best_params_)\n",
    "print(\"Best KNN F1 Score:\", knn_grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9c2bbe-64a3-4ef0-aab3-f96a3963d544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Machine (SVM)\n",
    "svm_params = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'kernel': ['linear', 'rbf'],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "svm_model = SVC()\n",
    "svm_grid = GridSearchCV(svm_model, svm_params, cv=skf, scoring='f1')\n",
    "svm_grid.fit(X, y)\n",
    "print(\"Best SVM Parameters:\", svm_grid.best_params_)\n",
    "print(\"Best SVM F1 Score:\", svm_grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f934a9-d595-415d-9cfc-855c2d75db55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "rf_params = {\n",
    "    'n_estimators': [50, 100, 150, 200],\n",
    "    'max_depth': [None, 10, 15, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10, 15, 20]\n",
    "}\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_grid = GridSearchCV(rf_model, rf_params, cv=skf, scoring='f1')\n",
    "rf_grid.fit(X, y)\n",
    "print(\"Best Random Forest Parameters:\", rf_grid.best_params_)\n",
    "print(\"Best Random Forest F1 Score:\", rf_grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba27948-95de-483e-9e99-b58accf2bf28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost parameters\n",
    "xgb_params = {\n",
    "    'n_estimators': [50, 100, 150, 200],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 8, 10]\n",
    "}\n",
    "xgb_model = XGBClassifier(eval_metric='logloss', random_state=42)\n",
    "xgb_grid = GridSearchCV(xgb_model, xgb_params, cv=skf, scoring='f1')\n",
    "xgb_grid.fit(X, y)\n",
    "\n",
    "print(\"Best XGBoost Parameters:\", xgb_grid.best_params_)\n",
    "print(\"Best XGBoost F1 Score:\", xgb_grid.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9dc0dab-c56b-4684-91ab-2c5aa3c6d9fe",
   "metadata": {},
   "source": [
    "SVM, Random Forest, and XGBoost all have an F1 score higher than KNN’s F1 score of 0.8424. \n",
    "Random Forest is the top performer here, albeit by a very small margin over SVM and XGBoost. \n",
    "Random Forest is easier to interpret than SVM and XGBoost because one can visualize the decision trees and assess feature importance. \n",
    "Given its high interpretability, Random Forest is chosen as the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b169bd-3114-4725-bad3-c343425e9015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example explain prediction\n",
    "# App"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2842e779-cd2f-484a-9e00-2aef09814ef6",
   "metadata": {},
   "source": [
    "### Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f025dfbb-5722-4ebd-bdd9-835927177252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best Random Forest model from GridSearchCV\n",
    "best_rf_model = rf_grid.best_estimator_\n",
    "\n",
    "# Retrieve feature importances\n",
    "feature_importances = best_rf_model.feature_importances_\n",
    "\n",
    "# Create a DataFrame for easy visualization\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': feature_importances\n",
    "})\n",
    "\n",
    "# Sort the DataFrame by importance\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Plot feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'])\n",
    "plt.gca().invert_yaxis()  # Invert y-axis to have the most important feature at the top\n",
    "plt.xlabel(\"Feature Importance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812258c8-c144-4d54-9de1-c25b6be864ef",
   "metadata": {},
   "source": [
    "`Credit_History`: This feature has the highest importance, suggesting that a customer's credit history significantly impacts the model's prediction, possibly indicating strong predictive power regarding loan approvals or similar financial decisions.\n",
    "\n",
    "`Total_Income_log` and `Balance_Income_log`: These income-related features also show notable importance, though they are less influential than Credit_History. They suggest that total and balanced income levels contribute significantly to the model’s decisions.\n",
    "\n",
    "`EMI_log`: The EMI feature holds moderate importance, indicating that the model considers the borrower's EMI burden in its predictions. Feature engineering assisted the model in forecasting the target variable. \n",
    "\n",
    "`Property_Area` and `Dependents`: These categorical variables have lower importance but still contribute to the model. They might represent factors such as urban vs. rural locations or family obligations affecting loan repayment capacity.\n",
    "\n",
    "`Married`,`Self_Employed`,`Self_Employed` and `Education`: These features have the least importance, indicating minimal impact on the model’s output. It suggests that marital status, employment type, gender, and education level are less critical for the model's predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8307d6-73ff-4f82-bafb-7ee7e6f33182",
   "metadata": {},
   "source": [
    "The model emphasizes financial history and income as primary predictors for lending decisions, while demographic features like gender, marital status, education, property area, and self-employed status are less influential and carry a higher risk of bias. \n",
    "\n",
    "**Recommendations for Removal of Demographic Features**\n",
    "- Gender: Low predictive power and potential for gender-based discrimination justify its removal.\n",
    "- Marital Status: Minimal relevance to creditworthiness; its removal helps avoid bias against single or non-traditional household setups.\n",
    "- Education: Does not directly impact repayment ability; removing it promotes fairness regardless of educational background.\n",
    "- Property Area: Could lead to geographic or socioeconomic bias; excluding it prevents discrimination based on location.\n",
    "- Self-Employed Status: Might disadvantage non-traditional workers, though it has limited influence on creditworthiness.\n",
    "\n",
    "\n",
    "**Ethical and Practical Justifications**\n",
    "- Fairness and Transparency: Removing these features supports ethical lending and aligns with fair lending practices, as they do not significantly affect model performance.\n",
    "- Explainability: Transparency about key features enhances trust and helps applicants understand eligibility factors.\n",
    "- Fairness Audits: Regular audits can detect and address potential biases, ensuring ongoing compliance with ethical standards and fair treatment across demographics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c0e3fc-e13f-4572-8d2c-c75e35ca97b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain the Random forest model with only financial history and income features\n",
    "\n",
    "feature_column_new = ['Credit_History', 'Total_Income_log', 'EMI_log', 'Balance_Income_log']\n",
    "\n",
    "X_new = X[feature_column_new]\n",
    "X_test_new = X_test[feature_column_new]\n",
    "\n",
    "# Retrieve the best parameters from the initial GridSearchCV\n",
    "best_rf_params = rf_grid.best_params_\n",
    "\n",
    "# Initialize a new RandomForestClassifier using the best parameters\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=best_rf_params['n_estimators'],\n",
    "    max_depth=best_rf_params['max_depth'],\n",
    "    min_samples_split=best_rf_params['min_samples_split'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit the model to the new data X1, y1\n",
    "rf_model.fit(X_new, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4db8ba5-865e-4e6d-bd5c-8ca3c267c9f4",
   "metadata": {},
   "source": [
    "The SHAP library is used to explain the classification of a loan application as either \"approved\" or \"rejected\" by identifying the contributions of specific features for that particular instance. This capability is crucial for financial institutions to build trust with their customers. By offering clear and understandable explanations for loan approval or rejection decisions using SHAP values, institutions can enhance transparency, trustworthiness, and customer satisfaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bae11a-7a42-4b8f-86d3-bd78ee53d08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the explainer\n",
    "explainer = shap.Explainer(rf_model.predict, X_test_new)\n",
    "\n",
    "# Calculates the SHAP values - It takes some time\n",
    "shap_values = explainer(X_test_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b954c575-c8bb-4ca5-a71a-c0eb46b9377b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on X_test_new\n",
    "y_pred = rf_model.predict(X_test_new.iloc[[9]])\n",
    "print('The model prediction is', y_pred[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b807081-1cfd-4887-a79a-837ca2140ac4",
   "metadata": {},
   "source": [
    "The Waterfall plot in the SHAP library is a visualization that helps explain the contribution of individual features to a specific prediction for a single instance (e.g., a single row of your dataset). It is particularly useful for understanding how the model arrives at a specific prediction by visualizing the impact of each feature value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c26e29-71ab-455c-81bd-b74f019fb45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.waterfall(shap_values[9])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f09afc-f467-46c0-b0a3-8f65ec084d34",
   "metadata": {},
   "source": [
    "Each bar represents a feature, and the direction of the bar (left or right) indicates whether the feature has a negative or positive impact on the prediction.\n",
    "\n",
    "- Features like `Balance_Income_log`, `Total_Income_log`, and `EMI_log` have significant negative contributions, pulling the prediction lower.\n",
    "- `Credit_History` has a positive contribution, slightly increasing the prediction.\n",
    "- The combined effect of all these features leads to the rejection of the loan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c1850e-27f3-48e7-8811-70209281554e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For completeness, these are the values of the original features before starting the analysis.\n",
    "test_data_original.iloc[9]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
